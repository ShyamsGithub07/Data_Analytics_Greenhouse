{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72c7f5c5-1e0b-4a1f-acdc-7f9497326a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting data from raw/greenhouse_raw.csv\n",
      "Rows: 720\n",
      "Saved cleaned data to data/greenhouse_cleaned.csv\n",
      "Detected 8 anomalies. Saved to data/anomalies.csv\n",
      "Cross-correlation failed: stem() got an unexpected keyword argument 'use_line_collection'\n",
      "Prophet not available — using RandomForest fallback.\n",
      "RF Forecast eval — MAE: 0.516, RMSE: 0.653, R2: 0.983\n",
      "RandomForest forecast saved to data/temperature_forecast.csv\n",
      "Report skeleton written to report/findings.md\n",
      "Pipeline complete. Outputs in /data, /models, /images, /report.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "greenhouse_analytics.py\n",
    "\n",
    "Full pipeline for Smart Greenhouse Environmental Analytics:\n",
    " - the first step involves in the process of ingesting raw hourly sensor data\n",
    " - the second step is cleaning & resampling\n",
    " - implementing Exploratory Data Analysis (EDA) & getting summary stats\n",
    " - Define ideal ranges & compute KPIs\n",
    " - Anomaly detection (rule-based, z-score, IsolationForest)\n",
    " - Cross-correlation checks\n",
    " - Short-term forecasting (Prophet preferred; RandomForest fallback)\n",
    " - Export cleaned data, anomalies, forecast, and brief report skeleton\n",
    "\n",
    "Author: Harish Ragavendra Chittibabu\n",
    "Date: 2024-12-16\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# 01) Imports & config\n",
    "# ----------------------------\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as matplt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.ensemble import IsolationForest, RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "\n",
    "# Prophet may be installed as 'prophet' or 'fbprophet'\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except Exception:\n",
    "    try:\n",
    "        from fbprophet import Prophet\n",
    "        PROPHET_AVAILABLE = True\n",
    "    except Exception:\n",
    "        PROPHET_AVAILABLE = False\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Paths & parameters\n",
    "# ----------------------------\n",
    "RAW_PATH = 'raw/greenhouse_raw.csv'\n",
    "CLEANED_PATH = 'data/greenhouse_cleaned.csv'\n",
    "ANOMALIES_PATH = 'data/anomalies.csv'\n",
    "FORECAST_PATH = 'data/temperature_forecast.csv'\n",
    "MODEL_PATH = 'models/temp_forecast.pkl'\n",
    "REPORT_PATH = 'report/findings.md'\n",
    "PLOTS_DIR = 'images/'\n",
    "\n",
    "IDEAL_RANGES = {\n",
    "    'temperature': (20.0, 28.0),    # °C\n",
    "    'humidity': (60.0, 80.0),       # %\n",
    "    'soil_moisture': (30.0, 60.0),  # %\n",
    "    'co2': (400.0, 1000.0),         # ppm\n",
    "}\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('report', exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: safe column check\n",
    "# ----------------------------\n",
    "def assert_columns(df, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing expected columns in data: {missing}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Ingest data\n",
    "# ----------------------------\n",
    "def ingest(path=RAW_PATH):\n",
    "    print('Ingesting data from', path)\n",
    "    df = pd.read_csv(path, parse_dates=['timestamp'])\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print('Rows:', len(df))\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Basic cleaning & resampling\n",
    "# ----------------------------\n",
    "def clean_and_resample(df, freq='1H'):\n",
    "    # Ensure expected columns exist\n",
    "    expected_cols = ['timestamp', 'temperature', 'humidity', 'soil_moisture', 'co2', 'light']\n",
    "    assert_columns(df, expected_cols)\n",
    "\n",
    "    # Drop duplicates by timestamp (keep last)\n",
    "    df = df.drop_duplicates(subset='timestamp', keep='last').copy()\n",
    "    df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "    # Ensure numeric columns\n",
    "    for c in ['temperature','humidity','soil_moisture','co2','light']:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "    # Resample to hourly regular grid (mean aggregation)\n",
    "    df = df.resample(freq).mean()\n",
    "\n",
    "    # Interpolate small gaps (<= 3 periods) using time interpolation\n",
    "    df.interpolate(method='time', limit=3, inplace=True)\n",
    "\n",
    "    # Optionally drop rows still full-NaN or mark them\n",
    "    df = df.dropna(how='all')\n",
    "\n",
    "    # Remove obviously invalid readings\n",
    "    df = df[(df['temperature'].between(-20, 60) | df['temperature'].isna())]\n",
    "    df = df[(df['humidity'].between(0, 100) | df['humidity'].isna())]\n",
    "\n",
    "    df.to_csv(CLEANED_PATH)\n",
    "    print('Saved cleaned data to', CLEANED_PATH)\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Resample aggregates (daily/weekly) & summary\n",
    "# ----------------------------\n",
    "def aggregates_and_summary(df):\n",
    "    daily = df.resample('D').agg(['mean','min','max','std'])\n",
    "    weekly = df.resample('W').mean()\n",
    "    summary = df.describe().T\n",
    "    # Save a basic summary plot\n",
    "    plt.figure(figsize=(10,6))\n",
    "    df[['temperature','humidity']].resample('D').mean().plot()\n",
    "    plt.title('Daily mean Temperature & Humidity')\n",
    "    plt.savefig(os.path.join(PLOTS_DIR,'daily_temp_humidity.png'), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return daily, weekly, summary\n",
    "\n",
    "# ----------------------------\n",
    "# 5) EDA: correlation & plots\n",
    "# ----------------------------\n",
    "def eda_plots(df):\n",
    "    # Correlation matrix\n",
    "    corr = df[['temperature','humidity','soil_moisture','co2','light']].corr()\n",
    "    matplt.figure(figsize=(8,6))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='vlag')\n",
    "    matplt.title('Sensor Correlation Matrix')\n",
    "    matplt.savefig(os.path.join(PLOTS_DIR,'correlation_matrix.png'), bbox_inches='tight')\n",
    "    matplt.close()\n",
    "\n",
    "    # Time series interactive-ready export (plotly optional in notebook)\n",
    "    # Save small sample plots\n",
    "    df[['temperature']].plot(figsize=(12,3), title='Temperature (Hourly)')\n",
    "    matplt.savefig(os.path.join(PLOTS_DIR,'temperature_ts.png'), bbox_inches='tight')\n",
    "    matplt.close()\n",
    "\n",
    "    return corr\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Define ideal ranges & KPIs\n",
    "# ----------------------------\n",
    "def compute_kpis(df, ideal_ranges=IDEAL_RANGES):\n",
    "    # Binary 'in_ideal' columns\n",
    "    for var, (lo, hi) in ideal_ranges.items():\n",
    "        if var in df.columns:\n",
    "            col = f'{var}_ok'\n",
    "            df[col] = df[var].between(lo, hi)\n",
    "\n",
    "    # Compute percent of time per day in ideal range for each metric\n",
    "    kpi_daily = {}\n",
    "    for var in ideal_ranges.keys():\n",
    "        col = f'{var}_ok'\n",
    "        if col in df.columns:\n",
    "            pct = df[col].resample('D').mean() * 100\n",
    "            kpi_daily[var] = pct\n",
    "            # Save a plot per KPI\n",
    "            matplt.figure(figsize=(10,2))\n",
    "            pct.plot(title=f'%time in ideal range per day: {var}')\n",
    "            matplt.ylabel('%')\n",
    "            matplt.savefig(os.path.join(PLOTS_DIR,f'kpi_{var}_daily.png'), bbox_inches='tight')\n",
    "            matplt.close()\n",
    "    return df, kpi_daily\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Anomaly detection\n",
    "# ----------------------------\n",
    "def detect_anomalies(df):\n",
    "    # RULE-BASED anomalies\n",
    "    df['temp_delta'] = df['temperature'].diff().abs()\n",
    "    df['anomaly_rule'] = (df['temperature'] > 35) | (df['temp_delta'] > 8)\n",
    "\n",
    "    # Z-SCORE anomalies for temperature (abs z > 3)\n",
    "    df['temp_z'] = (df['temperature'] - df['temperature'].mean()) / df['temperature'].std()\n",
    "    df['anomaly_z'] = df['temp_z'].abs() > 3\n",
    "\n",
    "    # Isolation Forest (multivariate)\n",
    "    features = ['temperature','humidity','soil_moisture','co2','light']\n",
    "    feat_df = df[features].fillna(method='ffill').fillna(method='bfill')\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "    iso_pred = iso.fit_predict(feat_df)\n",
    "    df['iso_anom'] = (iso_pred == -1)\n",
    "\n",
    "    # Final anomaly flag (OR) — you could also majority-vote\n",
    "    df['anomaly_final'] = df[['anomaly_rule','anomaly_z','iso_anom']].any(axis=1)\n",
    "\n",
    "    # Export anomalies with context\n",
    "    anoms = df[df['anomaly_final']].copy()\n",
    "    anoms['matched_rules'] = (\n",
    "        anoms[['anomaly_rule','anomaly_z','iso_anom']]\n",
    "        .apply(lambda row: ','.join([c for c,v in zip(['rule','zscore','iso'], row) if v]), axis=1)\n",
    "    )\n",
    "    anoms.to_csv(ANOMALIES_PATH)\n",
    "    print(f\"Detected {len(anoms)} anomalies. Saved to {ANOMALIES_PATH}\")\n",
    "    return df, anoms\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Relationship & cross-correlation checks\n",
    "# ----------------------------\n",
    "def relationship_checks(df, var_x='light', var_y='temperature', nlags=48):\n",
    "    # Fill NaNs for ccf calculation\n",
    "    x = df[var_x].fillna(0).values\n",
    "    y = df[var_y].fillna(0).values\n",
    "    # compute cross-correlation function (note: ccf returns non-negative lags correlation)\n",
    "    try:\n",
    "        cc = ccf(x, y)[:nlags]\n",
    "        # save small plot of cross-correlation\n",
    "        matplt.figure(figsize=(10,3))\n",
    "        matplt.stem(range(len(cc)), cc, use_line_collection=True)\n",
    "        matplt.title(f'Cross-correlation: {var_x} -> {var_y}')\n",
    "        matplt.xlabel('Lag')\n",
    "        matplt.savefig(os.path.join(PLOTS_DIR,f'ccf_{var_x}_{var_y}.png'), bbox_inches='tight')\n",
    "        matplt.close()\n",
    "        return cc\n",
    "    except Exception as e:\n",
    "        print('Cross-correlation failed:', e)\n",
    "        return None\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Forecasting: Prophet if available, else RandomForest\n",
    "# ----------------------------\n",
    "def forecast_temperature(df, horizon_hours=24*7):\n",
    "    # prepare series of temperature\n",
    "    temp = df['temperature'].reset_index().rename(columns={'timestamp':'ds','temperature':'y'})\n",
    "    temp = temp.dropna()\n",
    "\n",
    "    if PROPHET_AVAILABLE:\n",
    "        print(\"Using Prophet for forecasting.\")\n",
    "        m = Prophet(daily_seasonality=True, weekly_seasonality=True)\n",
    "        m.fit(temp)\n",
    "        future = m.make_future_dataframe(periods=horizon_hours, freq='H')\n",
    "        fcst = m.predict(future)\n",
    "        fcst_out = fcst[['ds','yhat','yhat_lower','yhat_upper']].rename(columns={'yhat':'yhat_temp'})\n",
    "        fcst_out.to_csv(FORECAST_PATH, index=False)\n",
    "        joblib.dump(m, MODEL_PATH)\n",
    "        print('Prophet forecast saved to', FORECAST_PATH)\n",
    "        return fcst_out\n",
    "    else:\n",
    "        # RandomForest time-series style forecasting using lag features\n",
    "        print(\"Prophet not available — using RandomForest fallback.\")\n",
    "        df_rf = df[['temperature']].copy()\n",
    "        # create lag features\n",
    "        for lag in [1,3,6,12,24,48]:\n",
    "            df_rf[f'temp_lag_{lag}'] = df_rf['temperature'].shift(lag)\n",
    "        # rolling features\n",
    "        df_rf['temp_roll_24'] = df_rf['temperature'].rolling(24).mean().shift(1)\n",
    "        df_rf = df_rf.dropna()\n",
    "\n",
    "        # predict next 24*horizon as single-step iterative forecast\n",
    "        # split train/test by time\n",
    "        train = df_rf.iloc[:-horizon_hours]\n",
    "        test = df_rf.iloc[-horizon_hours:]\n",
    "\n",
    "        X_train = train.drop(columns=['temperature'])\n",
    "        y_train = train['temperature']\n",
    "        X_test = test.drop(columns=['temperature'])\n",
    "        y_test = test['temperature']\n",
    "\n",
    "        rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        preds = rf.predict(X_test)\n",
    "\n",
    "        # Evaluate\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        print(f\"RF Forecast eval — MAE: {mae:.3f}, RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n",
    "\n",
    "        # Build forecast output aligned with test index\n",
    "        fcst_out = pd.DataFrame({\n",
    "            'ds': X_test.index,\n",
    "            'yhat_temp': preds\n",
    "        })\n",
    "        fcst_out.to_csv(FORECAST_PATH, index=False)\n",
    "        joblib.dump(rf, MODEL_PATH)\n",
    "        print('RandomForest forecast saved to', FORECAST_PATH)\n",
    "        return fcst_out\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Write a short findings report skeleton\n",
    "# ----------------------------\n",
    "def write_report(df, anoms, forecast):\n",
    "    lines = []\n",
    "    lines.append('# Greenhouse Analytics — Findings (AUTO-GENERATED)')\n",
    "    lines.append('')\n",
    "    lines.append('## Data summary')\n",
    "    lines.append(f'- Data period: {df.index.min()} to {df.index.max()}')\n",
    "    lines.append(f'- Total datapoints (hourly): {len(df)}')\n",
    "    lines.append('')\n",
    "    lines.append('## KPI (time in ideal ranges) — latest 7 days averages')\n",
    "    for var in IDEAL_RANGES.keys():\n",
    "        col = f'{var}_ok'\n",
    "        if col in df.columns:\n",
    "            pct = df[col].resample('D').mean().tail(7).mean() * 100\n",
    "            lines.append(f'- {var}: {pct:.1f}% time in ideal range (last 7 days)')\n",
    "    lines.append('')\n",
    "    lines.append('## Anomalies')\n",
    "    lines.append(f'- Total anomalies detected: {len(anoms)}')\n",
    "    lines.append('- Sample anomalies (timestamp, temperature, matched_rules):')\n",
    "    for idx, row in anoms.head(5).iterrows():\n",
    "        lines.append(f'  - {idx} | temp={row.get(\"temperature\")} | rules={row.get(\"matched_rules\")}')\n",
    "    lines.append('')\n",
    "    lines.append('## Forecast (next period)')\n",
    "    lines.append(f'- Forecast rows saved to: {FORECAST_PATH}')\n",
    "    lines.append('')\n",
    "    lines.append('## Recommendations')\n",
    "    lines.append('- Investigate repeated anomalies at same times — could be sensor drift or scheduled events.')\n",
    "    lines.append('- If temperature frequently exceeds ideal max, consider improving ventilation / shading.')\n",
    "    lines.append('- Use the forecast to preemptively adjust HVAC/irrigation schedules.')\n",
    "    with open(REPORT_PATH, 'w') as f:\n",
    "        f.write('\\n'.join(lines))\n",
    "    print('Report skeleton written to', REPORT_PATH)\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Main pipeline orchestration\n",
    "# ----------------------------\n",
    "# Define the path to your data file\n",
    "RAW_PATH = 'raw/greenhouse_raw.csv'  # This path needs to be updated\n",
    "\n",
    "# Make sure the file exists before running the code\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(RAW_PATH):\n",
    "        print(f\"Error: The file {RAW_PATH} does not exist.\")\n",
    "        print(\"Please ensure the file exists or update the RAW_PATH variable.\")\n",
    "        return  # Exit the function if file doesn't exist\n",
    "    \n",
    "    df_raw = ingest(RAW_PATH)\n",
    "    df_clean = clean_and_resample(df_raw)\n",
    "    daily, weekly, summary = aggregates_and_summary(df_clean)\n",
    "    corr = eda_plots(df_clean)\n",
    "    df_with_kpis, kpi_daily = compute_kpis(df_clean)\n",
    "    df_anom, anoms = detect_anomalies(df_with_kpis)\n",
    "    cc = relationship_checks(df_anom, var_x='light', var_y='temperature', nlags=48)\n",
    "    forecast = forecast_temperature(df_anom, horizon_hours=24*7)\n",
    "    write_report(df_anom, anoms, forecast)\n",
    "    print('Pipeline complete. Outputs in /data, /models, /images, /report.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaccec5-6dd1-4459-a94a-a7f66e88e564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
